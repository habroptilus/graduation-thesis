{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attentionベクトルをがんばってとりだすぞ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers          import Lambda, Input, Dense, GRU, LSTM, Dropout\n",
    "from keras.models          import Model\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.callbacks       import LambdaCallback \n",
    "from keras.optimizers      import Adam\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers.normalization import BatchNormalization as BN\n",
    "import keras.backend as K\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "import glob\n",
    "import copy\n",
    "import os\n",
    "import re\n",
    "import MeCab\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from gensim.models import word2vec\n",
    "import numpy as np\n",
    "from keras.utils import plot_model\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras import initializers\n",
    "model = word2vec.Word2Vec.load('/mnt/sdc/wikipedia_data/jawiki_wakati.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "addDict={}\n",
    "seq_len=13\n",
    "categories=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictVector(word, around_words_list):\n",
    "    global addDict\n",
    "    if word in addDict:\n",
    "        return addDict[word]\n",
    "    else:\n",
    "        return addUnknownWord(word,around_words_list)\n",
    "\n",
    "def addUnknownWord(word , around_words_list):\n",
    "    global addDict\n",
    "    rand_vector=np.random.rand(200)/np.linalg.norm(np.random.rand(200))*(10+ 3*np.random.rand(1))\n",
    "    vector=np.array(model[model.predict_output_word(around_words_list)[0][0]])+rand_vector\n",
    "    addDict[word]=vector\n",
    "    return vector\n",
    "    \n",
    "def Wakati(text):\n",
    "    m = MeCab.Tagger (\"-Ochasen -d /usr/lib/mecab/dic/mecab-ipadic-neologd -Owakati\")\n",
    "    result=m.parse(text)\n",
    "    ws = re.compile(\" \")\n",
    "    words = [word for word in ws.split(result)]\n",
    "    if words[-1] == u\"\\n\":\n",
    "        words = words[:-1]\n",
    "    return [word for word in words if word!=\"「\" and word!=\"」\" and word!=\"、\"]\n",
    "\n",
    "def extractKeyword(text,word_class):\n",
    "    tagger = MeCab.Tagger('-Ochasen')\n",
    "    tagger.parse('') # <= 空文字列をparseする\n",
    "    node = tagger.parseToNode(text)\n",
    "    keywords = []\n",
    "    while node:\n",
    "        if node.feature.split(\",\")[0] == word_class:\n",
    "            keywords.append(node.surface)\n",
    "        node = node.next\n",
    "    return keywords\n",
    "\n",
    "def seq2vecs(words,predict):\n",
    "    global addDict\n",
    "    vectors=[]\n",
    "    for i in range(len(words)):\n",
    "            try:\n",
    "                vectors.append(model[words[i]])\n",
    "            except:\n",
    "                if predict:\n",
    "                    try:\n",
    "                        vectors.append(predictVector(words[i],[words[i-1]]))\n",
    "                    except:\n",
    "                        if i==0:\n",
    "                            return []\n",
    "                        else:\n",
    "                            similar_word=model.similar_by_vector(addDict[words[i-1]], topn=10, restrict_vocab=None)[0][0]\n",
    "                            vectors.append(predictVector(words[i],[similar_word]))\n",
    "                else:\n",
    "                    return []\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_divide(X,Y,T,test_rate):\n",
    "    datanum=len(X)\n",
    "    n=math.floor(datanum*test_rate)\n",
    "    X_train=np.array(X[:datanum-n])\n",
    "    Y_train=np.array(Y[:datanum-n])\n",
    "    X_test=np.array(X[datanum-n:])\n",
    "    Y_test=np.array(Y[datanum-n:])\n",
    "    T_train=np.array(T[:datanum-n])\n",
    "    T_test=np.array(T[datanum-n:])\n",
    "    print(X_train.shape)\n",
    "    print(Y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(Y_test.shape)\n",
    "    print(len(T_train))\n",
    "    print(len(T_test))\n",
    "    return (X_train,Y_train,T_train),(X_test,Y_test,T_test)\n",
    "\n",
    "def onehot_vector(number):\n",
    "    global categories\n",
    "    onehot=np.zeros(categories)\n",
    "    onehot[number]=1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename,sfl,predict,extract):\n",
    "    global seq_len\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    T=[]\n",
    "    sum=0\n",
    "    max_len=0\n",
    "    min_len=1000000\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = [line for line in f]\n",
    "        for line in lines:\n",
    "            title,category=line.split(\" \")\n",
    "            words=[]\n",
    "            if extract:\n",
    "                words=extractKeyword(title,\"名詞\")\n",
    "            else:\n",
    "                words=Wakati(title)\n",
    "            input_vectors=seq2vecs(words,predict)\n",
    "            sum+=len(input_vectors)\n",
    "            max_len=max(max_len,len(input_vectors))\n",
    "            if len(input_vectors) > seq_len:\n",
    "                input_vectors=input_vectors[:seq_len]\n",
    "            elif len(input_vectors)==0:\n",
    "                continue\n",
    "            min_len=min(min_len,len(input_vectors))\n",
    "            if sfl:\n",
    "                random.shuffle(input_vectors)\n",
    "            x = [ [0.]*200 for _ in range(seq_len) ]\n",
    "            x[0:len(input_vectors)]=input_vectors\n",
    "            y=onehot_vector(int(category))\n",
    "            X.append(np.array(x))\n",
    "            Y.append(np.array(y))\n",
    "            T.append(title)\n",
    "    X,Y=shuffle(X,Y)\n",
    "    X=np.array(X)\n",
    "    Y=np.array(Y)\n",
    "    print(\"-------DataShape------\")\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    print(len(T))\n",
    "    print(\"-------DataProperties------\")\n",
    "    print(\"file:\"+filename)\n",
    "    print(\"max:\"+str(max_len))\n",
    "    print(\"min:\"+str(min_len))\n",
    "    print(\"mean:\"+str(sum/len(T)))\n",
    "    return X,Y,T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        self.W = K.variable(self.init((input_shape[-1],1)))\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x,self.W))\n",
    "        eij=K.squeeze(eij,axis=2)\n",
    "        ai = K.exp(eij)\n",
    "        Sum=K.expand_dims(K.sum(ai, axis=1),axis=1)\n",
    "        weights = ai/Sum\n",
    "        weights=K.expand_dims(weights,axis=1)\n",
    "        return weights\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hikaru/.pyenv/versions/anaconda3-4.4.0/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:40: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------DataShape------\n",
      "(68258, 13, 200)\n",
      "(68258, 6)\n",
      "68258\n",
      "-------DataProperties------\n",
      "file:./data/yahoo_data.txt\n",
      "max:13\n",
      "min:2\n",
      "mean:6.215784230419877\n"
     ]
    }
   ],
   "source": [
    "X,Y,T=load_data(\"./data/yahoo_data.txt\",sfl=False,predict=False,extract=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61433, 13, 200)\n",
      "(61433, 6)\n",
      "(6825, 13, 200)\n",
      "(6825, 6)\n",
      "61433\n",
      "6825\n"
     ]
    }
   ],
   "source": [
    "(X_train,Y_train,T_train),(X_test,Y_test,T_test)=train_test_divide(X,Y,T,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(args):\n",
    "    x,weights=args\n",
    "    weighted_input = K.batch_dot(weights, x)\n",
    "    weighted_input=K.squeeze(weighted_input,axis=1)\n",
    "    return weighted_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 13, 200)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 13, 1024)     2920448     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention (AttLayer)            (None, 13)           1024        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1024)         0           bidirectional_1[0][0]            \n",
      "                                                                 attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "predict (Dense)                 (None, 6)            6150        lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,927,622\n",
      "Trainable params: 2,927,622\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 43003 samples, validate on 18430 samples\n",
      "Epoch 1/1\n",
      "43003/43003 [==============================] - 130s 3ms/step - loss: 0.4845 - acc: 0.8330 - val_loss: 0.3602 - val_acc: 0.8760\n"
     ]
    }
   ],
   "source": [
    "inputs      = Input(shape=(seq_len, 200))\n",
    "x     = Bidirectional(LSTM(512,return_sequences=True,dropout=0.3))(inputs)\n",
    "weights    = AttLayer(name=\"attention\")(x)\n",
    "encoded =Lambda(encode,output_shape=(1024,))([x,weights])\n",
    "preds     = Dense(categories,activation='softmax',name=\"predict\")(encoded)\n",
    "LSTM_Model = Model(inputs=inputs, outputs=preds)\n",
    "LSTM_Model.summary()\n",
    "LSTM_Model.compile(optimizer=Adam(), loss='categorical_crossentropy',metrics=[\"accuracy\"])\n",
    "history =LSTM_Model.fit(X_train,Y_train,shuffle=True,batch_size=30,epochs=1,\n",
    "                   validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6825/6825 [==============================] - 6s 824us/step\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics_yahoo = LSTM_Model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " <Yahoo> loss:0.3581013316003394 accuracy:0.871941391941392\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n <Yahoo> loss:{} accuracy:{}\"\n",
    "      .format(loss_and_metrics_yahoo[0],loss_and_metrics_yahoo[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = \"attention\"\n",
    "intermediate_layer_model = Model(inputs=LSTM_Model.input,\n",
    "                                 outputs=LSTM_Model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(X_test)\n",
    "attention_vectors=intermediate_output.reshape(intermediate_output.shape[0],intermediate_output.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "米:0.18819253\n",
      "Google:0.19487444\n",
      "地図:0.20320328\n",
      "サービス:0.109083146\n",
      "を:0.03740421\n",
      "買収:0.033794228\n",
      "EOS:0.033435278\n",
      "EOS:0.03336167\n",
      "EOS:0.033338692\n",
      "EOS:0.033330247\n",
      "EOS:0.033327617\n",
      "EOS:0.033327468\n",
      "EOS:0.033327155\n",
      "----------------\n",
      "Apple:0.28072456\n",
      "物足りない:0.28085902\n",
      "iOS:0.054863084\n",
      "刷新:0.038112257\n",
      "EOS:0.039688975\n",
      "EOS:0.038744386\n",
      "EOS:0.038281262\n",
      "EOS:0.038166374\n",
      "EOS:0.038127933\n",
      "EOS:0.038113188\n",
      "EOS:0.038107783\n",
      "EOS:0.03810626\n",
      "EOS:0.038104996\n",
      "----------------\n",
      "低迷:0.21629919\n",
      "アップル:0.041298755\n",
      "巻き返し:0.04121396\n",
      "なるか:0.052997816\n",
      "EOS:0.059528414\n",
      "EOS:0.29434705\n",
      "EOS:0.04298648\n",
      "EOS:0.040136717\n",
      "EOS:0.04956867\n",
      "EOS:0.041521132\n",
      "EOS:0.040211592\n",
      "EOS:0.039975148\n",
      "EOS:0.039915092\n",
      "----------------\n",
      "KDDI:0.20762764\n",
      "通信:0.056700308\n",
      "障害:0.03685419\n",
      "客:0.02962886\n",
      "に:0.2076427\n",
      "700円:0.034572624\n",
      "返金:0.20760392\n",
      "EOS:0.07185859\n",
      "EOS:0.032928754\n",
      "EOS:0.029236063\n",
      "EOS:0.028589869\n",
      "EOS:0.028414888\n",
      "EOS:0.0283417\n",
      "----------------\n",
      "米:0.23257907\n",
      "Google:0.04153371\n",
      "地図:0.22078328\n",
      "サービス:0.031691145\n",
      "買収:0.053943865\n",
      "か:0.17693661\n",
      "EOS:0.0490567\n",
      "EOS:0.034412175\n",
      "EOS:0.032284755\n",
      "EOS:0.031819906\n",
      "EOS:0.03169067\n",
      "EOS:0.031645857\n",
      "EOS:0.03162224\n",
      "----------------\n",
      "アップル:0.19298534\n",
      "発表:0.19167368\n",
      "会:0.16958332\n",
      "の:0.081290826\n",
      "内容:0.031882305\n",
      "を:0.14222029\n",
      "予想:0.030106623\n",
      "EOS:0.027407078\n",
      "EOS:0.026746113\n",
      "EOS:0.026578106\n",
      "EOS:0.026524864\n",
      "EOS:0.026506156\n",
      "EOS:0.026495319\n",
      "----------------\n",
      "Pinterest:0.27851418\n",
      "で:0.276828\n",
      "人気:0.04407263\n",
      "写真:0.054727945\n",
      "と:0.03918849\n",
      "なる:0.040342145\n",
      "鍵:0.03832108\n",
      "EOS:0.03806116\n",
      "EOS:0.037999187\n",
      "EOS:0.03798294\n",
      "EOS:0.037982024\n",
      "EOS:0.037987594\n",
      "EOS:0.03799257\n",
      "----------------\n",
      "悪質:0.28705725\n",
      "な:0.04036632\n",
      "出会い系:0.04374365\n",
      "アプリ:0.27422106\n",
      "に:0.040086925\n",
      "注意:0.041657284\n",
      "EOS:0.039277285\n",
      "EOS:0.038986146\n",
      "EOS:0.038927425\n",
      "EOS:0.038913738\n",
      "EOS:0.03891441\n",
      "EOS:0.038921066\n",
      "EOS:0.038927518\n",
      "----------------\n",
      "KDDI:0.16353859\n",
      "一連:0.16206905\n",
      "の:0.04910687\n",
      "LTE:0.16399077\n",
      "障害:0.13369577\n",
      "を:0.16250423\n",
      "説明:0.029482616\n",
      "へ:0.023640484\n",
      "EOS:0.022617806\n",
      "EOS:0.022398133\n",
      "EOS:0.022335965\n",
      "EOS:0.022315487\n",
      "EOS:0.022304408\n",
      "----------------\n",
      "ドコモ:0.37238735\n",
      "通話:0.05386879\n",
      "定額:0.0509545\n",
      "報道:0.059868388\n",
      "を:0.05143146\n",
      "否定:0.050901785\n",
      "EOS:0.051632572\n",
      "EOS:0.050691307\n",
      "EOS:0.053701326\n",
      "EOS:0.051545862\n",
      "EOS:0.051083054\n",
      "EOS:0.05098262\n",
      "EOS:0.05095097\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    title=Wakati(T_test[i])+[\"EOS\"]*(seq_len-len(Wakati(T_test[i])))\n",
    "    attention_vector=attention_vectors[i]\n",
    "    for j in range(seq_len):\n",
    "        print(title[j]+\":\"+str(attention_vector[j]))\n",
    "    print(\"----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
