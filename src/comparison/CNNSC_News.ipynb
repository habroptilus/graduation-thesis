{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yoon kimのモデル(CNN for sentence classification) chainerで実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer import ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import numpy as np\n",
    "import chainer\n",
    "import chainer.links as L\n",
    "from chainer import optimizers, cuda, serializers\n",
    "import chainer.functions as F\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "from gensim.models import word2vec\n",
    "import MeCab\n",
    "import re\n",
    "word2vecModel = word2vec.Word2Vec.load('/mnt/sdc/wikipedia_data/jawiki_wakati.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "addDict={}\n",
    "seq_len=13\n",
    "categories=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リンク数を可変にしたいのでChainListを使用する\n",
    "class CNNSC(ChainList):\n",
    "    def __init__(self,input_channel,output_channel,filter_height,\n",
    "                 filter_width,n_label,max_sentence_len):\n",
    "        # フィルター数、使用されたフィルター高さ、最大文長は後から使う\n",
    "        self.cnv_num = len(filter_height)#フィルター数\n",
    "        self.filter_height = filter_height\n",
    "        self.max_sentence_len = max_sentence_len\n",
    "        \n",
    "        # Convolution層用のLinkをフィルター毎に追加\n",
    "        # Convolution2D(　入力チャンネル数, 出力チャンネル数（形毎のフィルターの数）, フィルターの形（タプル形式で）, パディングサイズ )\n",
    "        link_list = [L.Convolution2D(input_channel, output_channel, (i, filter_width), pad=0) for i in filter_height]\n",
    "        # Dropoff用のLinkを追加\n",
    "        link_list += [L.Linear(output_channel * self.cnv_num, output_channel * self.cnv_num)]\n",
    "        # 出力層へのLinkを追加\n",
    "        link_list += [L.Linear(output_channel * self.cnv_num, n_label)]\n",
    "\n",
    "        # ここまで定義したLinkのリストを用いてクラスを初期化する\n",
    "        super(CNNSC, self).__init__(*link_list)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # フィルタを通した中間層を準備\n",
    "        h_conv = [None for _ in self.filter_height]\n",
    "        h_pool = [None for _ in self.filter_height]\n",
    "        \n",
    "        # フィルタ形毎にループを回す\n",
    "        for i, filter_size in enumerate(self.filter_height):\n",
    "            # Convolition層を通す\n",
    "            h_conv[i] = F.relu(self[i](x))\n",
    "            # Pooling層を通す\n",
    "            h_pool[i] = F.max_pooling_2d(h_conv[i], (self.max_sentence_len+1-filter_size))\n",
    "        # Convolution+Poolingを行った結果を結合する\n",
    "        concat = F.concat(h_pool, axis=2)\n",
    "        # 結合した結果に対してDropoutをかける\n",
    "        h_l1 = F.dropout(F.tanh(self[self.cnv_num+0](concat)), ratio=0.5)\n",
    "        # Dropoutの結果を出力層まで圧縮する\n",
    "        y = self[self.cnv_num+1](h_l1)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train,Y_train,X_test,Y_test,batch_size,epochs):\n",
    "    global seq_len\n",
    "    global categories\n",
    "    batchsize   = batch_size    # minibatch size\n",
    "    n_epoch     = epochs        # エポック数\n",
    "    height=seq_len                  # length of sentences\n",
    "    width=200                  #size of wordembedding vecteor\n",
    "    in_units = seq_len\n",
    "    input_channel = 1\n",
    "    n_label =  categories# ラベル数\n",
    "    filter_height = [3,4,5] # フィルタの高さ\n",
    "    filter_width  = 200 # フィルタの幅 (embeddingの次元数)\n",
    "    output_channel = 100\n",
    "    decay = 0.0001 # 重み減衰\n",
    "    grad_clip = 3  # gradient norm threshold to clip\n",
    "    max_sentence_len = seq_len # max length of sentences\n",
    "    N=len(X_train)\n",
    "    N_test=len(X_test)\n",
    "\n",
    "    # モデルの定義\n",
    "    model = CNNSC(input_channel,output_channel,filter_height,\n",
    "                  filter_width,n_label,max_sentence_len)\n",
    "    # Setup optimizer\n",
    "    optimizer = optimizers.AdaDelta()\n",
    "    optimizer.setup(model)\n",
    "    optimizer.add_hook(chainer.optimizer.GradientClipping(grad_clip))\n",
    "    optimizer.add_hook(chainer.optimizer.WeightDecay(decay))\n",
    "    #gpuつかう\n",
    "    gpu_device = 0\n",
    "    cuda.check_cuda_available()\n",
    "    cuda.get_device(gpu_device).use()\n",
    "    model.to_gpu(gpu_device)\n",
    "    xp = cuda.cupy \n",
    "\n",
    "    # Learning loop\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "\n",
    "        print('epoch', epoch, '/', n_epoch)\n",
    "\n",
    "        # training\n",
    "        perm = np.random.permutation(len(X_train)) #ランダムな整数列リストを取得\n",
    "        sum_train_loss     = 0.0\n",
    "        sum_train_accuracy = 0.0\n",
    "        for i in range(0, N, batchsize):\n",
    "            \n",
    "            #perm を使い x_train, y_trainからデータセットを選択 (毎回対象となるデータは異なる)\n",
    "            x = chainer.Variable(xp.asarray(X_train[perm[i:i + batchsize]])) #source\n",
    "            t = chainer.Variable(xp.asarray(Y_train[perm[i:i + batchsize]])) #target\n",
    "\n",
    "            model.zerograds()\n",
    "            \n",
    "            y = model(x)\n",
    "            \n",
    "            loss = F.softmax_cross_entropy(y, t) # 損失の計算\n",
    "            accuracy = F.accuracy(y, t) # 正解率の計算\n",
    "\n",
    "            sum_train_loss += loss.data * len(t)\n",
    "            sum_train_accuracy += accuracy.data * len(t)\n",
    "\n",
    "            # 最適化を実行\n",
    "            loss.backward()\n",
    "            optimizer.update()\n",
    "\n",
    "        print('train mean loss={}, accuracy={}'.format(sum_train_loss / N, sum_train_accuracy / N)) #平均誤差\n",
    "\n",
    "        # evaluation\n",
    "        sum_test_loss     = 0.0\n",
    "        sum_test_accuracy = 0.0\n",
    "        for i in range(0, N_test, batchsize):\n",
    "\n",
    "            # all test data\n",
    "            x = chainer.Variable(xp.asarray(X_test[i:i + batchsize]))\n",
    "            t = chainer.Variable(xp.asarray(Y_test[i:i + batchsize]))\n",
    "            y = model(x)\n",
    "        \n",
    "            loss = F.softmax_cross_entropy(y, t) # 損失の計算\n",
    "            accuracy = F.accuracy(y, t) # 正解率の計算\n",
    "\n",
    "            sum_test_loss += loss.data * len(t)\n",
    "            sum_test_accuracy += accuracy.data * len(t)\n",
    "\n",
    "        print(' test mean loss={}, accuracy={}'.format(sum_test_loss / N_test, sum_test_accuracy / N_test)) #平均誤差\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83999,)\n",
      "(83999,)\n",
      "file: Yahoo\n",
      "(75600,)\n",
      "(75600,)\n",
      "(8399,)\n",
      "(8399,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hikaru/.pyenv/versions/anaconda3-4.4.0/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------DataShape------\n",
      "(61464, 1, 13, 200)\n",
      "(61464,)\n",
      "61464\n",
      "-------DataProperties------\n",
      "max:12\n",
      "min:2\n",
      "mean:6.161021085513472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hikaru/.pyenv/versions/anaconda3-4.4.0/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/hikaru/.pyenv/versions/anaconda3-4.4.0/envs/tensorflow/lib/python3.6/site-packages/gensim/models/word2vec.py:1463: UserWarning: All the input context words are out-of-vocabulary for the current model.\n",
      "  warnings.warn(\"All the input context words are out-of-vocabulary for the current model.\")\n",
      "/home/hikaru/.pyenv/versions/anaconda3-4.4.0/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: Call to deprecated `similar_by_vector` (Method will be removed in 4.0.0, use self.wv.similar_by_vector() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------DataShape------\n",
      "(8399, 1, 13, 200)\n",
      "(8399,)\n",
      "8399\n",
      "-------DataProperties------\n",
      "max:11\n",
      "min:2\n",
      "mean:6.0369091558518875\n"
     ]
    }
   ],
   "source": [
    "def predictVector(word, around_words_list):\n",
    "    global addDict\n",
    "    if word in addDict:\n",
    "        return addDict[word]\n",
    "    else:\n",
    "        return addUnknownWord(word,around_words_list)\n",
    "\n",
    "def addUnknownWord(word , around_words_list):\n",
    "    global addDict\n",
    "    rand_vector=np.random.rand(200)/np.linalg.norm(np.random.rand(200))*(10+ 3*np.random.rand(1))\n",
    "    vector=np.array(word2vecModel[word2vecModel.predict_output_word(around_words_list)[0][0]])+rand_vector\n",
    "    addDict[word]=vector\n",
    "    return vector\n",
    "    \n",
    "def Wakati(text):\n",
    "    m = MeCab.Tagger (\"-Ochasen -d /usr/lib/mecab/dic/mecab-ipadic-neologd -Owakati\")\n",
    "    result=m.parse(text)\n",
    "    ws = re.compile(\" \")\n",
    "    words = [word for word in ws.split(result)]\n",
    "    if words[-1] == u\"\\n\":\n",
    "        words = words[:-1]\n",
    "    return [word for word in words if word!=\"「\" and word!=\"」\" and word!=\"、\"and word!=\"。\"\n",
    "            and word!=\"!\" and word!=\"?\"]\n",
    "\n",
    "def seq2vecs(words,predict):\n",
    "    global addDict\n",
    "    vectors=[]\n",
    "    for i in range(len(words)):\n",
    "            try:\n",
    "                vectors.append(word2vecModel[words[i]])\n",
    "            except:\n",
    "                if predict:\n",
    "                    try:\n",
    "                        vectors.append(predictVector(words[i],[words[i-1]]))\n",
    "                    except:\n",
    "                        if i==0:\n",
    "                            continue\n",
    "                        else:\n",
    "                            similar_word=word2vecModel.similar_by_vector(addDict[words[i-1]], topn=10, restrict_vocab=None)[0][0]\n",
    "                            vectors.append(predictVector(words[i],[similar_word]))\n",
    "                else:\n",
    "                    return []\n",
    "    return vectors\n",
    "\n",
    "def train_test_divide(X,Y,test_rate):\n",
    "    datanum=len(X)\n",
    "    n=math.floor(datanum*test_rate)\n",
    "    X_train=np.array(X[:datanum-n])\n",
    "    Y_train=np.array(Y[:datanum-n])\n",
    "    X_test=np.array(X[datanum-n:])\n",
    "    Y_test=np.array(Y[datanum-n:])\n",
    "    print(X_train.shape)\n",
    "    print(Y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(Y_test.shape)\n",
    "    return (X_train,Y_train),(X_test,Y_test)\n",
    "\n",
    "def onehot_vector(number):\n",
    "    global categories\n",
    "    onehot=np.zeros(categories)\n",
    "    onehot[number]=1\n",
    "    return onehot\n",
    "\n",
    "def load_file(filename):\n",
    "    ttl=[]\n",
    "    cat=[]\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = [line for line in f]\n",
    "        for line in lines:\n",
    "            title,category=line.split(\" \")\n",
    "            ttl.append(title)\n",
    "            cat.append(int(category))\n",
    "    ttl,cat=shuffle(ttl,cat)\n",
    "    ttl=np.array(ttl)\n",
    "    cat=np.array(cat)\n",
    "    print(ttl.shape)\n",
    "    print(cat.shape)\n",
    "    return ttl,cat\n",
    "def create_data(ttl,cat,predict,sfl):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    T=[]\n",
    "    sum=0\n",
    "    max_len=0\n",
    "    min_len=1000000\n",
    "    for i in range(len(ttl)):\n",
    "        title=ttl[i]\n",
    "        category=cat[i]\n",
    "        words=Wakati(title)\n",
    "        input_vectors=seq2vecs(words,predict)\n",
    "        sum+=len(input_vectors)\n",
    "        max_len=max(max_len,len(input_vectors))\n",
    "        if len(input_vectors) > seq_len:\n",
    "            input_vectors=input_vectors[:seq_len]\n",
    "        elif len(input_vectors)==0:\n",
    "            continue\n",
    "        min_len=min(min_len,len(input_vectors))\n",
    "        if sfl:\n",
    "            random.shuffle(input_vectors)\n",
    "        x = [ [0.]*200 for _ in range(seq_len) ]\n",
    "        x[0:len(input_vectors)]=input_vectors\n",
    "        X.append(np.array([x],dtype=\"float32\"))\n",
    "        Y.append(int(category))\n",
    "        T.append(title)\n",
    "    X=np.array(X,dtype=\"float32\")\n",
    "    Y=np.array(Y)\n",
    "    print(\"-------DataShape------\")\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    print(len(T))\n",
    "    print(\"-------DataProperties------\")\n",
    "    print(\"max:\"+str(max_len))\n",
    "    print(\"min:\"+str(min_len))\n",
    "    print(\"mean:\"+str(sum/len(T)))\n",
    "    return X,Y,T\n",
    "def load_dataset(filename,sfl):\n",
    "    global seq_len\n",
    "    ttl,cat=load_file(filename)\n",
    "    if filename==\"./data/livedoor_data.txt\":\n",
    "        print(\"file: livedoor\")\n",
    "        X_test,Y_test,T_test=create_data(ttl,cat,predict=True,sfl=False)\n",
    "        return X_test,Y_test,T_test\n",
    "    else:\n",
    "        print(\"file: Yahoo\")\n",
    "        (train_ttl,train_cat),(test_ttl,test_cat)=train_test_divide(ttl,cat,0.1)\n",
    "        X_train,Y_train,T_train=create_data(train_ttl,train_cat,predict=False,sfl=False)\n",
    "        X_test,Y_test,T_test=create_data(test_ttl,test_cat,predict=True,sfl=False)\n",
    "        return (X_train,Y_train,T_train),(X_test,Y_test,T_test)\n",
    "    \n",
    "(X_train,Y_train,T_train),(X_test,Y_test,T_test)=load_dataset(\"./data/yahoo_data.txt\"\n",
    "                                                              ,sfl=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 5\n",
      "train mean loss=0.7230937, accuracy=0.7646102\n",
      " test mean loss=0.69633824, accuracy=0.7834266\n",
      "epoch 2 / 5\n",
      "train mean loss=0.53722525, accuracy=0.8279481\n",
      " test mean loss=0.6742136, accuracy=0.793785\n",
      "epoch 3 / 5\n",
      "train mean loss=0.48018828, accuracy=0.84727645\n",
      " test mean loss=0.71178114, accuracy=0.7904512\n",
      "epoch 4 / 5\n",
      "train mean loss=0.44130707, accuracy=0.8612196\n",
      " test mean loss=0.712445, accuracy=0.79330873\n",
      "epoch 5 / 5\n",
      "train mean loss=0.40860924, accuracy=0.8708024\n",
      " test mean loss=0.693857, accuracy=0.7992618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.CNNSC at 0x7f1ed750af98>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(X_train,Y_train,X_test,Y_test,batch_size=30,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
