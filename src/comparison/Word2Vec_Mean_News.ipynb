{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特徴ベクトルに、Word2Vecの平均(名詞のみ抽出)を用いた場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hikaru/.pyenv/versions/anaconda3-4.4.0/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers          import Lambda, Input, Dense, GRU, LSTM, Dropout\n",
    "from keras.models          import Model,Sequential\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.callbacks       import LambdaCallback \n",
    "from keras.optimizers      import Adam\n",
    "from keras.layers.normalization import BatchNormalization as BN\n",
    "import keras.backend as K\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "import glob\n",
    "import copy\n",
    "import os\n",
    "import re\n",
    "import MeCab\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from gensim.models import word2vec\n",
    "import numpy as np\n",
    "from keras.utils import plot_model\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "model = word2vec.Word2Vec.load('/mnt/sdc/wikipedia_data/jawiki_wakati.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=6\n",
    "addDict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractKeyword(text,word_class):\n",
    "    tagger = MeCab.Tagger('-Ochasen')\n",
    "    tagger.parse('') # <= 空文字列をparseする\n",
    "    node = tagger.parseToNode(text)\n",
    "    keywords = []\n",
    "    while node:\n",
    "        if node.feature.split(\",\")[0] == word_class:\n",
    "            keywords.append(node.surface)\n",
    "        node = node.next\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83999,)\n",
      "(83999,)\n",
      "file: Yahoo\n",
      "(75600,)\n",
      "(75600,)\n",
      "(8399,)\n",
      "(8399,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hikaru/.pyenv/versions/anaconda3-4.4.0/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------DataShape------\n",
      "(68817, 200)\n",
      "(68817, 6)\n",
      "68817\n",
      "-------DataProperties------\n",
      "max:13\n",
      "min:1\n",
      "mean:5.3839022334597555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hikaru/.pyenv/versions/anaconda3-4.4.0/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/hikaru/.pyenv/versions/anaconda3-4.4.0/envs/tensorflow/lib/python3.6/site-packages/gensim/models/word2vec.py:1463: UserWarning: All the input context words are out-of-vocabulary for the current model.\n",
      "  warnings.warn(\"All the input context words are out-of-vocabulary for the current model.\")\n",
      "/home/hikaru/.pyenv/versions/anaconda3-4.4.0/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: Call to deprecated `similar_by_vector` (Method will be removed in 4.0.0, use self.wv.similar_by_vector() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------DataShape------\n",
      "(8395, 200)\n",
      "(8395, 6)\n",
      "8395\n",
      "-------DataProperties------\n",
      "max:12\n",
      "min:1\n",
      "mean:5.345205479452055\n"
     ]
    }
   ],
   "source": [
    "def predictVector(word, around_words_list):\n",
    "    global addDict\n",
    "    if word in addDict:\n",
    "        return addDict[word]\n",
    "    else:\n",
    "        return addUnknownWord(word,around_words_list)\n",
    "\n",
    "def addUnknownWord(word , around_words_list):\n",
    "    global addDict\n",
    "    rand_vector=np.random.rand(200)/np.linalg.norm(np.random.rand(200))*(10+ 3*np.random.rand(1))\n",
    "    vector=np.array(model[model.predict_output_word(around_words_list)[0][0]])+rand_vector\n",
    "    addDict[word]=vector\n",
    "    return vector\n",
    "    \n",
    "def Wakati(text):\n",
    "    m = MeCab.Tagger (\"-Ochasen -d /usr/lib/mecab/dic/mecab-ipadic-neologd -Owakati\")\n",
    "    result=m.parse(text)\n",
    "    ws = re.compile(\" \")\n",
    "    words = [word for word in ws.split(result)]\n",
    "    if words[-1] == u\"\\n\":\n",
    "        words = words[:-1]\n",
    "    return [word for word in words if word!=\"「\" and word!=\"」\" and word!=\"、\"and word!=\"。\"\n",
    "            and word!=\"!\" and word!=\"?\"]\n",
    "\n",
    "def seq2vecs(words,predict):\n",
    "    global addDict\n",
    "    vectors=[]\n",
    "    for i in range(len(words)):\n",
    "            try:\n",
    "                vectors.append(model[words[i]])\n",
    "            except:\n",
    "                if predict:\n",
    "                    try:\n",
    "                        vectors.append(predictVector(words[i],[words[i-1]]))\n",
    "                    except:\n",
    "                        if i==0:\n",
    "                            continue\n",
    "                        else:\n",
    "                            similar_word=model.similar_by_vector(addDict[words[i-1]], topn=10, restrict_vocab=None)[0][0]\n",
    "                            vectors.append(predictVector(words[i],[similar_word]))\n",
    "                else:\n",
    "                    return []\n",
    "    return vectors\n",
    "\n",
    "def train_test_divide(X,Y,test_rate):\n",
    "    datanum=len(X)\n",
    "    n=math.floor(datanum*test_rate)\n",
    "    X_train=np.array(X[:datanum-n])\n",
    "    Y_train=np.array(Y[:datanum-n])\n",
    "    X_test=np.array(X[datanum-n:])\n",
    "    Y_test=np.array(Y[datanum-n:])\n",
    "    print(X_train.shape)\n",
    "    print(Y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(Y_test.shape)\n",
    "    return (X_train,Y_train),(X_test,Y_test)\n",
    "\n",
    "def onehot_vector(number):\n",
    "    global categories\n",
    "    onehot=np.zeros(categories)\n",
    "    onehot[number]=1\n",
    "    return onehot\n",
    "\n",
    "def load_file(filename):\n",
    "    ttl=[]\n",
    "    cat=[]\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = [line for line in f]\n",
    "        for line in lines:\n",
    "            title,category=line.split(\" \")\n",
    "            ttl.append(title)\n",
    "            cat.append(int(category))\n",
    "    ttl,cat=shuffle(ttl,cat)\n",
    "    ttl=np.array(ttl)\n",
    "    cat=np.array(cat)\n",
    "    print(ttl.shape)\n",
    "    print(cat.shape)\n",
    "    return ttl,cat\n",
    "\n",
    "def create_data(ttl,cat,predict,sfl):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    T=[]\n",
    "    sum=0\n",
    "    max_len=0\n",
    "    min_len=1000000\n",
    "    for i in range(len(ttl)):\n",
    "        title=ttl[i]\n",
    "        category=cat[i]\n",
    "        words=extractKeyword(title,\"名詞\")\n",
    "        input_vectors=seq2vecs(words,predict)\n",
    "        sum+=len(input_vectors)\n",
    "        max_len=max(max_len,len(input_vectors))\n",
    "        if len(input_vectors)==0:\n",
    "            continue\n",
    "        min_len=min(min_len,len(input_vectors))\n",
    "        if sfl:\n",
    "            random.shuffle(input_vectors)\n",
    "        x=list(np.average(input_vectors,axis=0))    \n",
    "        y=onehot_vector(int(category))\n",
    "        X.append(np.array(x))\n",
    "        Y.append(np.array(y))\n",
    "        T.append(title)\n",
    "    X=np.array(X)\n",
    "    Y=np.array(Y)\n",
    "    print(\"-------DataShape------\")\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    print(len(T))\n",
    "    print(\"-------DataProperties------\")\n",
    "    print(\"max:\"+str(max_len))\n",
    "    print(\"min:\"+str(min_len))\n",
    "    print(\"mean:\"+str(sum/len(T)))\n",
    "    return X,Y,T\n",
    "def load_dataset(filename,sfl):\n",
    "    ttl,cat=load_file(filename)\n",
    "    if filename==\"./data/livedoor_data.txt\":\n",
    "        print(\"file: livedoor\")\n",
    "        X_test,Y_test,T_test=create_data(ttl,cat,predict=True,sfl=False)\n",
    "        return X_test,Y_test,T_test\n",
    "    else:\n",
    "        print(\"file: Yahoo\")\n",
    "        (train_ttl,train_cat),(test_ttl,test_cat)=train_test_divide(ttl,cat,0.1)\n",
    "        X_train,Y_train,T_train=create_data(train_ttl,train_cat,predict=False,sfl=False)\n",
    "        X_test,Y_test,T_test=create_data(test_ttl,test_cat,predict=True,sfl=False)\n",
    "        return (X_train,Y_train,T_train),(X_test,Y_test,T_test)\n",
    "    \n",
    "(X_train,Y_train,T_train),(X_test,Y_test,T_test)=load_dataset(\"./data/yahoo_data.txt\"\n",
    "                                                              ,sfl=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               25728     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 34,374\n",
      "Trainable params: 34,374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 61935 samples, validate on 6882 samples\n",
      "Epoch 1/3\n",
      "61935/61935 [==============================] - 15s 238us/step - loss: 0.6084 - acc: 0.7876 - val_loss: 0.5532 - val_acc: 0.8053\n",
      "Epoch 2/3\n",
      "61935/61935 [==============================] - 11s 181us/step - loss: 0.5029 - acc: 0.8233 - val_loss: 0.5326 - val_acc: 0.8172\n",
      "Epoch 3/3\n",
      "61935/61935 [==============================] - 11s 182us/step - loss: 0.4600 - acc: 0.8388 - val_loss: 0.5075 - val_acc: 0.8242\n"
     ]
    }
   ],
   "source": [
    "#モデル作成\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=200,activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(categories, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy',metrics=[\"accuracy\"])\n",
    "#モデル学習\n",
    "history =model.fit(X_train,Y_train,shuffle=True,batch_size=30,epochs=3,\n",
    "                   validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a8489d1127d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_history' is not defined"
     ]
    }
   ],
   "source": [
    "#plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8395/8395 [==============================] - 1s 63us/step\n",
      "\n",
      "loss:0.5467719480776942 accuracy:0.819178082262781\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(X_test,Y_test)\n",
    "print(\"\\nloss:{} accuracy:{}\".format(loss_and_metrics[0],loss_and_metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
